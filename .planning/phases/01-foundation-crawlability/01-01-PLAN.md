---
phase: 01-foundation-crawlability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/pages/Index.tsx
  - public/robots.txt
  - public/sitemap.xml
  - public/_redirects
autonomous: true

must_haves:
  truths:
    - "React Helmet renders a dynamic <title> and <meta description> in the document head on the homepage"
    - "Visiting /robots.txt returns plain text (not HTML) containing a Sitemap directive"
    - "Visiting /sitemap.xml returns valid XML listing all indexable pages with priority values"
    - "Visiting a non-existent URL like /does-not-exist returns HTTP 404 status (not 200)"
  artifacts:
    - path: "src/pages/Index.tsx"
      provides: "Helmet usage on homepage proving the pattern works"
      contains: "import { Helmet } from \"react-helmet\""
    - path: "public/robots.txt"
      provides: "Crawl directives for search engines"
      contains: "Sitemap:"
    - path: "public/sitemap.xml"
      provides: "Complete page listing for search engine crawlers"
      contains: "<urlset"
    - path: "public/_redirects"
      provides: "Netlify routing with proper 404 catch-all"
      contains: "/* /index.html 404"
  key_links:
    - from: "public/robots.txt"
      to: "public/sitemap.xml"
      via: "Sitemap: URL reference"
      pattern: "Sitemap:.*sitemap\\.xml"
    - from: "public/sitemap.xml"
      to: "src/App.tsx routes"
      via: "URL listing matches all defined routes"
      pattern: "<loc>https://www\\.10xvelocity\\.ai/"
    - from: "public/_redirects"
      to: "public/index.html"
      via: "Explicit route-to-index.html rewrites with 404 catch-all last"
      pattern: "/\\*.*404"
---

<objective>
Install the React Helmet pattern on the homepage (proving FIX-03 infrastructure works) and create the three static files that make the site crawlable: robots.txt, sitemap.xml, and a restructured _redirects file that returns proper HTTP 404 for unknown URLs.

Purpose: Search engines currently cannot discover pages (no sitemap), receive no crawl directives (no robots.txt), and get soft 200 status for non-existent URLs. This plan fixes all three crawlability problems and establishes the Helmet pattern that Phase 2 will extend to every page.

Output: Four modified/created files that make the site properly crawlable.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-crawlability/01-RESEARCH.md

Key source files:
@src/pages/Index.tsx
@src/pages/IndustryTools.tsx (reference pattern for Helmet usage)
@src/App.tsx (complete route list)
@public/_redirects (current single-rule file)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add React Helmet to the homepage and create crawlability files</name>
  <files>
    src/pages/Index.tsx
    public/robots.txt
    public/sitemap.xml
  </files>
  <action>
1. **Add Helmet to Index.tsx** -- Follow the exact pattern from `src/pages/IndustryTools.tsx`:
   - Add `import { Helmet } from "react-helmet";` at the top
   - Wrap the existing JSX in a fragment `<>...</>` if not already wrapped
   - Add a `<Helmet>` block as the first child with:
     ```tsx
     <Helmet>
       <title>AI & Automation Consulting | 10x Velocity</title>
       <meta
         name="description"
         content="10x Velocity helps businesses automate workflows and leverage AI to save time and cut costs. Based in Louisville, KY."
       />
     </Helmet>
     ```
   - Do NOT change any other existing JSX in the file

2. **Create `public/robots.txt`** with this exact content:
   ```
   User-agent: *
   Allow: /

   Sitemap: https://www.10xvelocity.ai/sitemap.xml
   ```

3. **Create `public/sitemap.xml`** listing all routes from `src/App.tsx`. Use `https://www.10xvelocity.ai` as the base URL. Include every route EXCEPT the catch-all `*` (404). Assign priorities as follows:
   - Homepage `/`: priority 1.0, changefreq weekly
   - Top-level pages (`/about`, `/services`, `/contact`, `/case-studies`, `/blog`, `/industry-tools`, `/savings-calculator`): priority 0.8, changefreq monthly
   - Sub-pages (individual services, case studies, blog posts, events, programs): priority 0.6, changefreq monthly
   - Legal pages (`/privacy-policy`, `/terms-of-service`): priority 0.3, changefreq yearly
   - Utility pages (`/demo`, `/lexi-file`, `/power-automate`, `/prototypes`): priority 0.5, changefreq monthly

   The complete route list (31 routes total):
   ```
   /
   /about
   /services
   /services/data-cleaning
   /services/phone-voice-agents
   /services/ai-workshops
   /services/smart-bots
   /lexi-file
   /industry-tools
   /power-automate
   /prototypes
   /programs/ai-guide-certification
   /case-studies
   /case-studies/innes-young
   /case-studies/ecatalyst
   /case-studies/hillcrest-partners
   /case-studies/catalyst-group
   /case-studies/director-of-marketing
   /case-studies/birchwood-real-estate
   /case-studies/govbrokers
   /case-studies/inspyrd
   /case-studies/transportation-director
   /savings-calculator
   /events/lunch-and-learn
   /demo
   /blog
   /blog/1
   /blog/2
   /blog/3
   /blog/4
   /blog/5
   /contact
   /privacy-policy
   /terms-of-service
   ```

   Add an XML comment at the top: `<!-- Generated for 10x Velocity. Update when routes change in src/App.tsx -->`

   The XML must be valid: proper `<?xml?>` declaration, `<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">` namespace, and each route as a `<url>` with `<loc>`, `<priority>`, and `<changefreq>`.
  </action>
  <verify>
    - Run `npm run build` to confirm the project builds without errors
    - Verify `dist/robots.txt` exists after build (Vite copies public/ to dist/)
    - Verify `dist/sitemap.xml` exists after build
    - Count the `<url>` entries in sitemap.xml -- should be 33 (31 routes from App.tsx + privacy-policy + terms-of-service, which are already in the list, so 31 total)
  </verify>
  <done>
    - Index.tsx imports Helmet and renders a unique title and meta description
    - robots.txt exists in public/ with User-agent, Allow, and Sitemap directives
    - sitemap.xml exists in public/ with all 31 indexable routes, valid XML structure, and correct priorities
    - `npm run build` succeeds
  </done>
</task>

<task type="auto">
  <name>Task 2: Restructure _redirects for proper HTTP 404 status</name>
  <files>
    public/_redirects
  </files>
  <action>
Replace the current single-line `public/_redirects` file (which contains only `/* /index.html 200`) with an explicit route listing. This is CRITICAL for proper HTTP 404 status on Netlify.

**Why:** The current catch-all `/* /index.html 200` returns HTTP 200 for ALL URLs, including non-existent ones (soft 404). Search engines treat these as valid pages. The fix is to explicitly list every valid SPA route as a 200 rewrite, then use `/* /index.html 404` as the final catch-all.

**Important:** Netlify processes the FIRST matching rule. Static files in the publish directory (robots.txt, sitemap.xml, favicon.ico) are served by Netlify's shadowing mechanism BEFORE redirect rules are checked, so they do not need redirect entries.

Write this exact content to `public/_redirects`:

```
# Netlify redirects - first match wins
# Static files (robots.txt, sitemap.xml, favicon.ico, etc.) served automatically via shadowing

# SPA routes - serve index.html with 200 status
/                                    /index.html   200
/about                               /index.html   200
/services                            /index.html   200
/services/*                          /index.html   200
/lexi-file                           /index.html   200
/industry-tools                      /index.html   200
/power-automate                      /index.html   200
/prototypes                          /index.html   200
/programs/*                          /index.html   200
/case-studies                        /index.html   200
/case-studies/*                      /index.html   200
/savings-calculator                  /index.html   200
/events/*                            /index.html   200
/demo                                /index.html   200
/blog                                /index.html   200
/blog/*                              /index.html   200
/contact                             /index.html   200
/privacy-policy                      /index.html   200
/terms-of-service                    /index.html   200
/smart-bots                          /index.html   200

# Catch-all: any unmatched route returns 404 status
/*                                   /index.html   404
```

Note: Wildcard routes like `/services/*` cover all sub-routes (e.g., `/services/data-cleaning`, `/services/phone-voice-agents`). This keeps the file concise while covering all routes defined in `src/App.tsx`.
  </action>
  <verify>
    - Run `npm run build` and confirm `dist/_redirects` contains the new content (not just the old single line)
    - Count the lines: should have ~20 route rules plus the 404 catch-all
    - Confirm the LAST non-comment line ends with `404`
  </verify>
  <done>
    - `public/_redirects` explicitly lists all valid SPA routes with 200 status
    - The final catch-all rule uses 404 status instead of 200
    - All routes from src/App.tsx are covered (either explicitly or via wildcard patterns)
    - `npm run build` succeeds and dist/_redirects has the correct content
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `npm run build` succeeds without errors
2. `dist/robots.txt` exists and contains "Sitemap: https://www.10xvelocity.ai/sitemap.xml"
3. `dist/sitemap.xml` exists and contains valid XML with 31 `<url>` entries
4. `dist/_redirects` has explicit route listings ending with a 404 catch-all
5. `npm run dev` starts successfully, and visiting `http://localhost:8080/` shows the correct page title in the browser tab ("AI & Automation Consulting | 10x Velocity")
</verification>

<success_criteria>
- React Helmet renders dynamic meta tags on the homepage (verifiable by viewing page source or browser tab title)
- robots.txt is a plain text file (not HTML) with sitemap reference
- sitemap.xml is valid XML listing all 31 indexable pages with priorities
- _redirects file will return HTTP 404 for unknown URLs when deployed to Netlify (verifiable locally by checking file content; actual HTTP status requires deployment)
- The project builds without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-crawlability/01-01-SUMMARY.md`
</output>
